# -*- coding: utf-8 -*-
"""fermilab_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iAUHFU1Dior5mZuUqIEPZShJqGoPNffc
"""

import time
import tensorflow as tf
import keras
import pickle
#---IMPORTING DATABASES---
import psycopg2
import sys, os
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
#---IMPORTING KERAS---
from keras import optimizers
from keras.models import Model, Sequential, load_model
from keras.layers import Input, Dense, Dropout, Activation, LSTM, Embedding
from keras.utils import np_utils, to_categorical, plot_model
from keras.callbacks import ModelCheckpoint
#---COLAB SPECIFIC---
import pprint
from google.colab import files, drive
drive.mount('/content/drive')

class WindowGenerator(object):
  def __init__(self, xfiles, yfiles, batch_size, save_name, vocab):
    self.x_files = xfiles
    self.y_files = yfiles
    self.batch_size = batch_size
    self.saved = str(save_name)
    self.current_index = 0
    self.vocabulary = vocab
  def generate(self):
    x = np.zeros((self.batch_size, 40))
    y = np.zeros((self.batch_size, 41))
    while True:
      for i in range(self.batch_size):
        if self.current_index >= len(self.x_files):
          self.current_index = 0
        x[i,:] = self.x_files[self.current_index]
        for j in range(10):
          index = int(self.y_files[i,j])
          y[i, index] = 1
        self.current_index += 1
      yield x,y

def renumber_window(x, y):
  new_x = x
  new_y = y
  dictionary = vocab(new_x, {})
  new_x = renumber(new_x, dictionary)
  new_y = renumber(new_y, dictionary)
    
  #format dictionary into array
  names = ['def_id', 'index']
  formats = ['int64', 'int64']
  dtype = dict(names = names, formats = formats)
  dict_array = np.array(list(dictionary.items()), dtype = dtype)
    
  return new_x, new_y, dict_array      
            
#this function generates the windows of access usage cut into hour-long batches
#given the files accessed the previous hour, check to see if they're still
#running (i.e. not closed) in the specified hour, if so, add them to window.
#then start checking the files with an open time greater than the hour
#and cutoff at the start of the next hour.
def generate_num_windows(files, start_idx, out_length):
  dummy_files = np.copy(files)
  if (start_idx + out_length) >= len(files):
     start_idx = len(files) - out_length
  out_x = dummy_files[start_idx: (start_idx + out_length - 10)]
  out_y = dummy_files[(start_idx + out_length - 10):(start_idx + out_length)]
  final_x, final_y, dictionary = renumber_window(out_x, out_y)
  return out_x, out_y, dictionary

#this function generates the hour-long batches for the entirety
#of two-month window and saves it to the computer
def generate_all_hours(files):
  out_len = 50
  #since the windows are of variable length, I can't use a numpy array.
  #So I'll have a list of arrays !
  x = np.ones((1, (out_len - 10)))
  y = np.ones((1, 10))
  dictionaries = [] #different sizes means must be stored as nested list
  index = 0
  start_idx = 0
  #while you haven't run til the end of the files
  while (start_idx) <= len(files):
    #generate an hour-long window
    x_win, y_win, dictionary = generate_num_windows(files, start_idx, out_len)
    #assign the array
    if x_win != [] or y_win != []:
      x_ids = x_win
      y_ids = y_win
    else:
      x_ids = len(files) #an invalid id!
      y_ids = len(files)
    #insert at index position the new array of ids
    x = np.insert(x, index, x_ids, axis = 0)
    y = np.insert(y, index, y_ids, axis = 0)
    dictionaries.append(dictionary)
    #this is just a progress tracker
    print(str(start_idx), len(files))
    #increment index and hour
    index += 1
    start_idx += (out_len/5)
  #remove dummy first entry
  x = np.delete(x, len(x) -1, 0)
  y = np.delete(y, len(y) - 1, 0)
  return x, y, dictionaries

#this function saves all the hour-long batches and loads them when needed
def generate_all_batches(files, proj_id):
  #saved path
  x_path = "./drive/My Drive/batches/" + str(proj_id) + '_x.npz'
  y_path = "./drive/My Drive/batches/" + str(proj_id) + '_y.npz'
  dict_path = "./drive/My Drive/" + str(proj_id) + '_dict'
  if os.path.exists(x_path) and os.path.exists(y_path) and os.path.exists(dict_path):
    x = np.load(x_path, allow_pickle = True)
    y = np.load(y_path, allow_pickle = True)
    all_x = x['arr_0']
    all_y = y['arr_0']
    with open(dict_path, 'rb') as f:
      dictionary = pickle.load(f)
  #if file isn't saved, then generate, save, and return
  else:
    hour = 0
    all_x, all_y, dictionary = generate_all_hours(files)
    np.savez(x_path, all_x)
    np.savez(y_path, all_y)
    with open(dict_path, 'wb') as f:
      pickle.dump(dictionary, f)
  return all_x, all_y, dictionary

#this function connects to the samread database and pulls relevant
#data based on the start_date and end_date from the consumed_files
#and project_snapshots datasets
def load_data(ht, database, proj_id, start_date, end_date):
  try:
    #reader only access has same user/pass
    conn = psycopg2.connect(dbname = database, user = "samread", 
                            password = 'reader', host = ht, 
                            port = '5433', sslmode = 'disable')
    cur = conn.cursor()
    #gather all project_definitions from distinct process ids
    cur.execute("select distinct on (c.process_id) ps.proj_def_id, " +
                "process_id from consumed_files c join project_" +
                "snapshots as ps on ps.proj_snap_id = c.proj_snap_id" +
                " where (open_time between '" + start_date +
                "' and '" + end_date + "');")
    project_definitions = np.asarray(cur.fetchall())
    #gather the min and max open/close time for distinct process ids
    cur.execute("select distinct process_id, extract (epoch from min " +
                " (open_time)), extract (epoch from max (close_time)) " +
                "from consumed_files c where (open_time between '"
                + start_date + "' and '" + end_date + "')" +
                " group by process_id;")
    process_times = np.asarray(cur.fetchall())
    #make sure that both arrays are the same length
    #(they should be!)
    if (len(project_definitions) == len(process_times)):
      #if so, concatenate like so:
      #(project def, process id, open time, close time)
      files = np.concatenate((project_definitions, 
                              process_times[:,[1, 2]]), axis = 1)
    else:
      #error code if lengths don't match
      print("Error while concatenating data from NumPy arrays")
      sys.exit(0)
    #save files to computer
    np.savez(str(proj_id), files)    
  except (Exception, psycopg2.Error) as error:
    print ("Error while fetching data from PostgreSQL", error)
    sys.exit(1)
  finally:
    if (conn):
      cur.close()
      conn.close()
  return files

#this function finds the start and end dates to use for load data
#first, connect to the samread database, find the original date for
#the proj_id, calculate the two-month prior date.
def get_dates(ht, database, proj_id):
  try:
    conn = psycopg2.connect(dbname = database, user = "samread", 
                            password = "reader", host = ht, 
                            port = '5433', sslmode = 'disable')
    cur = conn.cursor()
    #connect and extract last_refresh_date 
    #from entry matching project definition id
    cur.execute("select extract (epoch from ps.last_refresh_date) as " +
                "last_refresh_date from project_snapshots ps " +
                "where ps.proj_def_id = '" + str(proj_id) + "';")
    arr = np.asarray(cur.fetchall())
    #error code: if there's no date available
    if arr == []:
      print("No data available for last_refresh_date for " + proj_name)
      sys.exit(1)
    else:
      #end date should be the last_refresh_date
      end_date = time.strftime('%Y-%m-%d', time.localtime(arr[0]))
      #start_date is 2 months before end date
      start_date = time.strftime('%Y-%m-%d', time.localtime(arr[0] - 5259486))
  #error code if connection fails
  except (Exception, psycopg2.Error) as error:
    print("Error while fetching data from PostgreSQL (get_dates)", error)
    sys.exit(1)
  finally:
    #close connection when done
    if (conn):
      cur.close()
      conn.close()
  #return all dates
  return start_date, end_date

#this function subracts the minimum time from every entry,
#meaning that the first entry in files now has time 0
#and increases from there. Kinda, sorta normalizes data ... at least
#it doesn't start in the millions now?
def normalize_time(files, initial_time):
  new_files = files[files[:,2].argsort()]
  #now subract that from from every open time!
  new_files[:, 2] -= initial_time
  for i in new_files:
    #if there's a valid close time, subract initial time
    if (i[3] is not None):
      i[3] -= initial_time
    #else if close time is blank/not valid,
    #just make the close time, the entry's open
    #time + 10 seconds
    else:
      i[3] = i[2] + 10
  return new_files

#this function merges consecutive occurences sharing the same project id
#reduces entries by appox. half
def merge(files):
  #initialize variables
  out = []
  current_pid, current_begin, current_end = files[0]
  last_begin = current_begin
  idx = 0
  for pid, t_begin, t_end in files:
  #criteria for creating a new interval: different id, first entry, 
  #or out of time bounds (15 minutes)
    new_interval = (pid!=current_pid or idx == 0  or t_begin>(last_begin+900))
    if new_interval:
      #append to out before reseting current pid/begin/end values
      out.append((current_pid, current_begin, current_end))
      current_pid, current_begin, current_end = pid, t_begin, t_end
    else:
      #min and max out the start and end values
      current_begin, current_end = min(t_begin, current_begin), max(t_end,current_end)
      #increment index and advance last_begin
      idx += 1
      last_begin = t_begin
  #append the last entry
  out.append((current_pid, current_begin, current_end))
  #turn list into array
  arr = np.asarray(out)
  return arr

#this function applies all the time functions to the dataset.
#the output of this function are the files stripped of the process_id
#ordered by open time, merged, and set to an open_time of 0.
def manipulate_time (files, initial_time):
  new_files = normalize_time(files, initial_time)
  #now I'm ordering the files, grouping them by project_def_id
  new_files = new_files[new_files[:,0].argsort()]
  #new_files = merge_time_by_pdid(new_files)
  #getting rid of distinct process ID going from 4 to 3 columns
  new_files = np.delete(new_files, 1, axis = 1)
  new_files = merge(new_files)
  #now sorting by open time
  new_files = new_files[new_files[:,1].argsort()]
  return new_files

#this function creates a dictionary of distinct project_definition_ids
def vocab(files, dictionary):
  index = 0
  for i in files:
    #if the project id isn't found already in the dictionary
    if dictionary.get(i) == None:
      #add it!
      dictionary[i] = index
      #but then increment the index
      index += 1
  return dictionary

#this function renumbers the file definition id 
#according to what's in the dictionary
def renumber(files, dictionary):
  for i in range(len(files)):
    value = dictionary.get(files[i])
    if value == None:
      files[i] = len(dictionary)
    else:
      files[i] = value
  return files

def accuracy_metric(model, X_true, y_true, epochs, batch_size, pid):
  table = np.zeros((epochs, 2))
  split = epochs - 10
  total_avg = 0
  if split >= 0:
    for i in range(1, 10):
      #this is the path to which the model would be saved ... so check here
      data_path = str(pid) + "_model-0" + str(i) + ".hdf5"
      print(data_path)
      print(os.path.exists(data_path))
      #if path exists, load it
      if os.path.exists(data_path):
        set_model = load_model(data_path)
      plt_table, avg = calculate_accuracy(X_true, y_true, set_model, batch_size)
      total_avg += avg
      table[i, 0] = i
      table[i, 1] = avg
    if split > 0:
      for i in range(10, epochs):
         #this is the path to which the model would be saved ... so check here
        data_path = str(pid) + "_model-" + str(i) + ".hdf5"
        print(data_path)
        print(os.path.exists(data_path))
        #if path exists, load it
        if os.path.exists(data_path):
          set_model = load_model(data_path)
        plt_table, avg = calculate_accuracy(X_true, y_true, set_model, batch_size)
        total_avg += avg
        table[i, 0] = i
        table[i, 1] = avg
  if split < 0:
    for i in range(0, epochs):
      #this is the path to which the model would be saved ... so check here
      data_path =  str(pid) + "_model-" + str(i) + ".hdf5"
      print(data_path)
      print(os.path.exists(data_path))
      #if path exists, load it
      if os.path.exists(data_path):
        set_model = load_model(data_path)
      plt_table, avg = calculate_accuracy(X_true, y_true, set_model, batch_size)
      total_avg += avg
      table[i, 0] = i
      table[i, 1] = avg
  print(table)
  flt_avg = float(total_avg)/float(epochs)
  plt.title('Overall Accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Match Accuracy')
  plt.plot(table[:,0], table[:,1])
  plt.show()
  print(flt_avg)
  return plt, flt_avg


#this function loads and manipulates the train and valid files!
def prepare_files(ht, database, proj_id):
  path1 = "./drive/My Drive/" + str(proj_id) + '.npz'
  #if saved file exists, load it
  if os.path.exists(path1):
    train = np.load(path1, allow_pickle = True)
    files = train['arr_0']
  #if not, get dates, and load data directly from psql. save at the end!
  else:
    start_date, end_date = get_dates(ht, database, proj_id)
    files = load_data(ht, database, proj_id, start_date, end_date)
    
  times = files[files[:,2].argsort()]
  initial_time = times[0,2]
  #manipulate time for each set of files before returning
  files = manipulate_time(files, initial_time)
  
  #whittle down to order list of project definition ids
  files = files[:,0]
  #find final window and dictionary
  relevant_files = files[(len(files) - 40): (len(files) - 1)]
  final_dictionary = vocab(relevant_files, {})
  #add project id as last "valid" entry in dictionary
  final_dictionary.update({proj_id: len(final_dictionary)})
        
  #create final window to use for final prediction
  final_window = np.insert(relevant_files, 39, proj_id, axis =0)
  final_window = renumber(final_window, final_dictionary)
    
  X, y, dictionaries = generate_all_batches(files, proj_id)
    
  final_cpy = np.copy(X[(len(X) - 40): (len(X) - 1)])
  final = np.insert(final_cpy, len(final_cpy) - 1, final_window, axis = 0)

  #now split into train, validation, and test files
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, shuffle = True)
  X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, shuffle = True)

  #reverse dictionary for final prediction call
  final_dictionary = {y:x for x,y in final_dictionary.iteritems()}
  
  return X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, final_dictionary, dictionaries, final

#this function builds the Long Short-Term Memory model for the data. This puts
#together all the previous functions to build the best fitting model etc etc etc
def fit_lstm(proj_id, epochs, batch_size, 
             X_train, X_valid, X_test, 
             y_train, y_valid, y_test, vocabulary):
  train_windows = WindowGenerator(X_train, y_train, batch_size, 
                                  (str(proj_id) + 'train'), vocabulary)
  valid_windows = WindowGenerator(X_valid, y_valid, batch_size, 
                                  (str(proj_id) + 'valid'), vocabulary)
  test_windows = WindowGenerator(X_test, y_test, batch_size, 
                                 (str(proj_id) + 'test'), vocabulary)

  main_input = Input(shape = (40,), dtype = 'int64', name = 'main_input')
  x = Embedding(40, 6, input_length = 40)(main_input)
  lstm_out = LSTM(50, activation = 'relu', dropout =0.2, 
                 recurrent_dropout = 0.2, return_sequences = False)(x)
  final = Dense(100)(lstm_out)
  drop = Dropout(0.3)(final)
  main_output = Dense(41, activation = 'sigmoid', name = 'main_output')(drop)
  model = Model(inputs = [main_input], outputs =[main_output])
  print(model.summary())
    
  model.compile(loss='mse', optimizer = 'adadelta', metrics =['accuracy'])
  #saves the epoch versions of the model
  checkpointer = ModelCheckpoint(filepath = (str(proj_id) + '_model-{epoch:02d}.hdf5'), 
                                 verbose = 1)
  #use fit_generator to load the extreme amount of data into the model
  history = model.fit_generator(train_windows.generate(), 
                            steps_per_epoch = len(X_train)/batch_size,
                            epochs = epochs,verbose =1,
                            validation_steps = len(X_valid)/batch_size,
                            validation_data = valid_windows.generate(),
                            callbacks = [checkpointer])
  model.evaluate_generator(test_windows.generate(), verbose = 1, 
                           steps = len(X_test)/batch_size)
  # Plot training & validation loss values
  plt.plot(history.history['acc'])
  plt.plot(history.history['val_acc'])
  plt.title('Model accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper left')
  plt.show()
  # Plot training & validation loss values
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('Model loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper left')
  plt.show()
  return model
  
#This function calculates the accuracy stat for a set of predicted y
#vs actual y. Used in the retroactive accuracy_metric function too
def y_calc (prediction, actual):
  print(prediction, actual)
  #average accuracy per categories 1-41
  calc = np.zeros((1, 41))
  #actual is fed in format [2, 6, 8] etc wherease y_actual is an empty
  #zero array of length 41, but all the indexes mentioned in actual are
  #represented as ones in y_actual
  y_actual = np.zeros((1, 41))
  #actual is of length 10
  for i in range(10):
    #so change the value of y_actual from 0 to 1 at these 10 specified indices
    y_actual[0, int(actual[i])] = 1
  #now find the accuracy per category
  for i in range(41):
    out = 0
    if y_actual[0,i] == 0:
      out = 1 - prediction[i]
    if y_actual[0,i] == 1:
      out = prediction[i]
    calc[0,i] = out
  #average out all accuracies and return
  avg = np.average(calc)
  return avg
 
#this function calculates the accuracy of the last batch
def calculate_accuracy(X, y, model, batch_size):
  print(model)
  end_idx = batch_size
  plt_table = np.zeros((len(y), 2))
  while end_idx < len(X):
    prediction = model.predict_on_batch(X[end_idx - batch_size: end_idx])
    for i in range(batch_size):
      place =  i + end_idx - batch_size
      plt_table[place, 0] = place
      plt_table[place, 1] = y_calc(prediction[i], y[place])
    end_idx += batch_size
  avg = np.average(plt_table[:,1])
  print("Average accuracy: ", avg)
  return plt_table, avg
  
def rank_and_respond(arr, dictionary):
  ordered = np.sort(arr)[::-1]
  printout ="Predicted definitions: "
  for i in range(41):
    place = np.where(arr == ordered[i])
    if len(place[0]) > 1:
      printout += "Tie. "
      print(arr)
      break
    else:
      idx = float(str(place[0]).lstrip('[').rstrip(']'))
      
    if i < len(dictionary):
      printout += str(dictionary.get(idx))
    if ordered[i] >= 0.15 and i < len(dictionary):
      printout += "(Y), "
    if (ordered[i] <= 0.15) and i < len(dictionary):
      printout += "(N), "
  final = printout[:-2]
  return final
  
#this function takes a model, runs it, and spits out a prediction on what
#the next project_definition_id accessed will be. num = number of dummy iterations
#the generator will run. pred = number of predictions desired
def run_lstm(ht, db, pid, epochs, num, batch_size):
  #load necessary files for generator
  X, y, Xtr, Xv, Xte, ytr, yv, yte, fdict, diction, fwindow = prepare_files(ht, db, pid)
  #this is the path to which the model would be saved ... so check here
  data_path = str(pid) + "_model-0" + str(epochs) + ".hdf5"
  print(os.path.exists(data_path))
  #if path exists, load it
  if os.path.exists(data_path):
    model = load_model(data_path)
  #else create a model
  else:
    model = fit_lstm(pid, epochs, batch_size, 
                     Xtr, Xv, Xte, ytr, yv, yte, 
                     vocabulary = 41)

  #calculate overall accuracy of model
  accuracy_table, avg = calculate_accuracy(X, y, model, batch_size)
  overall_accuracy, flt_avg = accuracy_metric(model, X, y, epochs, batch_size, pid)
    
  #start message print out
  pred_print_out = "Predicted next project definitions: "

  #now starts calculating predictions
  for i in range(num):
    prediction = model.predict_on_batch(fwindow)
    #take the last outcome
    predict_def = prediction[39, :]
    print(predict_def)
    #add it to print statement
    pred_print_out += rank_and_respond(predict_def, fdict)
    pred_print_out +=  " "     
  print(pred_print_out)
  
  return predict_def
  
output = run_lstm("sampgsdb01", "sam_nova_prd", 3630559, 25, 1, 500) #07/01/2019
output2 = run_lstm("sampgsdb01", "sam_nova_prd", 2940619, 25, 1, 20) #08/11/2018
output3 = run_lstm("sampgsdb01", "sam_nova_prd", 1877555, 25, 1, 20) #2017